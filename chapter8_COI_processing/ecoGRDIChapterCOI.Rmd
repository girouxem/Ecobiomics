---
title: "COI Sample Processing Using DADA2"
author: "Emily Giroux"
date: "01/07/2021"
output: pdf_document
fontsize: 11pt
geometry: margin=1in
urlcolor: blue
header-includes: \usepackage{xcolor}
---
```{r, global_options, eval=TRUE, echo=FALSE, cache=TRUE}
library("knitr")
opts_chunk$set(tidy.opts=list(width.cutoff = 80), tidy = TRUE, fig.align = 'center',
               cache = FALSE, collapse = TRUE, echo = FALSE, eval = FALSE, include = FALSE,
               message = FALSE, quietly = TRUE, results = 'hide', warn.conflicts = FALSE, 
               warning = FALSE)
```

**Using package `BiocManager` to install required packages:**
```{r, biocInstall, eval=TRUE, echo=TRUE, include=TRUE, cache=TRUE, tidy=FALSE, message=FALSE}
r <- getOption("repos")
r["CRAN"] <- "http://cran.us.r-project.org"
options(repos = r)

if (!requireNamespace("BiocManager"))
    install.packages("BiocManager")
BiocManager::install()

library("BiocManager")
.cran_packages <- c("cowplot", "data.table", "ggplot2", "knitr", "rprojroot")
.bioc_packages <- c("BiocStyle", "Biostrings", "dada2", 
                    "ShortRead")
.inst <- .cran_packages %in% installed.packages()
if(any(!.inst)) {
   install.packages(.cran_packages[!.inst])
}
.inst <- .bioc_packages %in% installed.packages()
if(any(!.inst)) {
  BiocManager::install(.bioc_packages[!.inst], ask = FALSE)
}
```

**Load packages into session, and print package versions:**
```{r, showBiocPackages, echo=TRUE, eval=TRUE, include=TRUE, results='hold', cache=TRUE}
sapply(c(.cran_packages, .bioc_packages), require, character.only = TRUE)
```


```{r sourcing_my_functions, echo=FALSE, eval=TRUE, include=FALSE, cache=TRUE}
#Source our custom R scripts:    
#For this we will use the rprojroot package to set the directory structures. This will help us when finding our files to source functions. We specify ours is an RStudio project. The root object contains a function that will help us locate our package R files regarless of our current working directory.
library("rprojroot")
root <- rprojroot::is_rstudio_project
scriptsPath <- root$make_fix_file(".")("R")
scripts  <- dir(root$find_file("R", path = root$find_file()))
scriptsl <- paste(scriptsPath, scripts, sep = "//")
lapply(scriptsl, source)
# Record the path to the environment images directory:
sharedPath <- "/isilon/cfia-ottawa-fallowfield/users/girouxeml/PIRL_working_directory/"
analysis <- "ecobiomics/"
sharedPathAn <- paste(sharedPath, analysis, sep = "")
imageDirPath <- "/home/CFIA-ACIA/girouxeml/GitHub_Repos/r_environments/ecobiomics/"
startUpImage <- "ecobiomics_StartUp.RData"
```
\pagebreak    
**Load the saved image from Chapter 1**, then save it as a separate image to retain environment data specific to the COI processing and analysis workflow.
```{r, loadBaseImage, echo=TRUE, eval=FALSE, include=TRUE, results='hold'}
load(paste(imageDirPath, startUpImage, sep = ""))
chptImageA <- "ecobiomics_COI_full.RData"
save.image(paste(imageDirPath, chptImageA, sep = ""))
```
When re-starting a session, you can quickly load up the image by running the chunk below:
```{r, quickImageLoad, echo=TRUE, eval=TRUE, include=TRUE, results='hold'}
sharedPath <- "/isilon/cfia-ottawa-fallowfield/users/girouxeml/PIRL_working_directory/"
analysis <- "ecobiomics/"
sharedPathAn <- paste(sharedPath, analysis, sep = "")
imageDirPath <- "/home/CFIA-ACIA/girouxeml/GitHub_Repos/r_environments/ecobiomics/"
chptImageA   <- "ecobiomics_COI_full.RData"
load(paste(imageDirPath, chptImageA, sep = ""))
```

**Create a variable shortcut to the region-specific analysis directory:**    
In this chapter, we are working with the COI amplicon samples, so the COI directory within our main project directory will contain all the region-specific output.
```{r, setRegDir, echo=TRUE, eval=FALSE, include=TRUE, cache=TRUE}
region <- "COI"
sharedPathReg <- paste(sharedPathAn, region, "_Full_analysis/", sep = "")
if(!dir.exists(sharedPathReg)) dir.create(sharedPathReg)

# Make the region-specific metadatable for this chapter the main metadata table:
metadataRegion <- metadataCOI
```
\pagebreak    
**Sample processing follows the steps outlined in Chapter 1**    
    
1. ggPlotRaw COI:
```{r ggPlotRaw, eval=TRUE, include=TRUE, echo=TRUE, fig.show='hold', message=FALSE, tidy=FALSE, cache=TRUE}
library("dada2")
library("ggplot2")
plotRawQFwd <- dada2::plotQualityProfile(paste(metadataRegion$rawFastqPath[1:2], 
                                               metadataRegion$rawR1[1:2], sep = "")) +
  ggplot2::ggtitle("Quality Plot of Two Samples of Unprocessed Forward Reads") +
  ggplot2::theme(plot.title = element_text(hjust = 0.5))

plotRawQRev <- dada2::plotQualityProfile(paste(metadataRegion$rawFastqPath[1:2], 
                                               metadataRegion$rawR2[1:2], sep = "")) +
  ggplot2::ggtitle("Quality Plot of Two Samples of Unprocessed Reverse Reads") +
  ggplot2::theme(plot.title = element_text(hjust = 0.5))

library("cowplot")
cowplot::plot_grid(plotRawQFwd, plotRawQRev, nrow = 2)
```
\pagebreak
     
**Checking for Adapter Sequences**     
We're going to use Cutadapt to detect and trim off COI-specific adapter sequences. Cutadapt uses Biostrings, which only recognises the main IUPAC codes. We need to replace any extended IUPAC nucleic acid ambiguity codes to the primary codes. In our COI primers we see the following ambiguiity codes:           
D is for A or G or T     
H is for A or C or T     
R is for A or G     
W is for A or T     
Y is for C or T     

I is from the extended IUPAC codes, and gives a probability order for C, T or G. The closest resemblance to the main IUPAC for I is B.     
     
**Note:**     
There were several sets of COI gene-specific primers listed in an Excel sheet in Ian's folder, and I'm not sure which ones were used for sequencing. Here I just checked which primer sets returned the most hits to see if there was an obvious set used for this sequencing data.     

COI gene-specific primers    
    
COI-Fish
COI-FishF1: TCAACCAACCACAAAGACATTGGCAC
COI-Fish330R: AGNGGGGGRTANACNGTTCA
                 Forward Complement Reverse RevComp
FWD.ForwardReads       0          0       0       0
FWD.ReverseReads       0          0       0       0
REV.ForwardReads       0          0       0       0
REV.ReverseReads       0          0       0       0    
    
COI-F: ATGATHGGDGCDCCWGAYATG
COI-R: CCWCCHCCHGCDGGRTC 
                 Forward Complement Reverse RevComp
FWD.ForwardReads     828          0       0     207
FWD.ReverseReads     193          0       0    1547
REV.ForwardReads       0          0       0       0
REV.ReverseReads       0          0       0       0    
    
    
COI-Invertebrates:
F230: GGTCAACAAATCATAAAGATATTGG
F230R_modN: CTTATRTTRTTTATNCGNGGRAANGC
                 Forward Complement Reverse RevComp
FWD.ForwardReads    6185          0       0     441
FWD.ReverseReads     493          0       0    3312
REV.ForwardReads     406          0       0    2693
REV.ReverseReads    5463          0       0     366  
    
    
COI-Invertebrates:
F230: GGTCAACAAATCATAAAGATATTGG
F230frag-230-R: CTTATRTTRTTTATICGIGGRAAIGC = CTTATRTTRTTTATBCGBGGRAABGC
                 Forward Complement Reverse RevComp
FWD.ForwardReads    6185          0       0     441
FWD.ReverseReads     493          0       0    3312
REV.ForwardReads     208          0       0    1305
REV.ReverseReads    2599          0       0     184

COI-Invertebrates: Bfrag:
Befrag-BF: CCIGAYATRGCITTYCCICG = CCBGAYATRGCBTTYCCBCG
Befrag-R5-R: GTRATIGCICCIGCIARIAC = GTRATBGCBCCBGCBARBAC
                 Forward Complement Reverse RevComp
FWD.ForwardReads    1076          0       0     234
FWD.ReverseReads     213          0       0    2270
REV.ForwardReads       0          0       0       0
REV.ReverseReads       0          0       0       0
          
**Results of Primer Check:**     
The most primer hits for pre-2022 sets came from the COI-Invertebrates set F230 with F230R_modN - processing will continue with this set first. The 2022 sets had the most hits agaisnt the Befrag-BF with Befrag-R5-R, so they need to be processed with that set.     
      

No changes are required for the forward or reverse primers as there are no extended ambiguity codes in the primer sequences:    
**F230:**     
     
>\textcolor{blue}{5'} GGTCAACAAATCATAAAGATATTGG \textcolor{blue}{3'}          
     
**F230R_modN:**      
     
>\textcolor{blue}{5'} CTTATRTTRTTTATNCGNGGRAANGC \textcolor{blue}{3'}           



2. Cutadapt. We are replacing the code for SeqPrep with the use of cutadapt and workflow developed by Benjamin Callahan. See page: https://benjjneb.github.io/dada2/ITS_workflow.html      
    
    
Before checking our COI sequences for primers with cutadapt, we need to remove those sequences with ambiguous bases. Ambiguous bases (Ns) in the sequencing reads makes accurate mapping of short primer sequences difficult, so we "pre-filter" these sequences so that we only remove those with Ns and perform no other fitlering.
```{r, filterNsfqs, echo=TRUE, eval=FALSE, include=TRUE, tidy=FALSE, cache=TRUE}
library("dada2")
# Path to the processed fastq file directory:
processedFastq <- paste(sharedPathReg, "processedFQ", sep = "")
if(!dir.exists(processedFastq)) dir.create(processedFastq)

# Path to fastq filtered for ambiguous bases (Ns):
filtNsPath <- file.path(processedFastq, "A_removedAmbiguous")
if(!dir.exists(filtNsPath)) dir.create(filtNsPath)

# Define path and file names:
metadataRegion$fwd   <- paste(metadataRegion$rawFastqPath, metadataRegion$rawR1, sep = "")
metadataRegion$filtF <- file.path(filtNsPath, 
                                  paste(metadataRegion$LibraryName, "_F.fastq.gz", sep = ""))
metadataRegion$rev   <- paste(metadataRegion$rawFastqPath, metadataRegion$rawR2, sep = "")
metadataRegion$filtR <- file.path(filtNsPath, 
                                  paste(metadataRegion$LibraryName, "_R.fastq.gz", sep = ""))

# Run DADA2's filterAndTrim function to remove sequences with ambiguous bases:
dada2::filterAndTrim(metadataRegion$fwd, metadataRegion$filtF, 
                     metadataRegion$rev, metadataRegion$filtR, maxN = 0, 
                     multithread = TRUE, verbose = TRUE)
```      

Capture your forward and reverse COI primers, F230 with F230R_modN, for pre-2022 sets:
```{r, recordPrimers, echo=TRUE, eval=TRUE, include=TRUE, tidy=FALSE, cache=TRUE}
fwdPrimer <- "GGTCAACAAATCATAAAGATATTGG"
revPrimer <- "CTTATRTTRTTTATNCGNGGRAANGC"
```

Create the custom `AllOrients' function for primer sequences for all possible orientations. This function was created by Benjamin Callahan for the ITS-specific workflow. See: https://benjjneb.github.io/dada2/ITS_workflow.html      
```{r, mkAllOrientsFn, echo=TRUE, eval=TRUE, include=TRUE, tidy=FALSE, cache=TRUE}
library("Biostrings")
AllOrients   <- function(primer) {
     require(Biostrings)
     dna     <- Biostrings::DNAString(primer)
     orients <- c(Forward    = dna, 
                  Complement = Biostrings::complement(dna), 
                  Reverse    = Biostrings::reverse(dna), 
                  RevComp    = Biostrings::reverseComplement(dna))
    return(sapply(orients, toString))
}
```

We can now use the custom AllOrients function to generate the primer sequences in all possible orientations in which they may be found:
```{r, runAllOrientsFn, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE, cache=TRUE, comment=NA}
fwdOrients <- AllOrients(fwdPrimer)
revOrients <- AllOrients(revPrimer)
fwdOrients
```

Create the custom `PrimerHits' function for checking our sequences for all orientations of primer sequences as generated above using the AllOrients function. This function generates a table of counts of the number of reads in which the primer is found. This function was created by Benjamin Callahan for the ITS-specific workflow but we can use it here as well. See: https://benjjneb.github.io/dada2/ITS_workflow.html
```{r, mkPrimerHitsFn, echo=TRUE, eval=TRUE, include=TRUE, tidy=FALSE, cache=TRUE}
library("ShortRead")
PrimerHits <- function(primer, fn) {
    nhits  <- Biostrings::vcountPattern(primer, ShortRead::sread(readFastq(fn)),
                                        fixed = FALSE)
    return(sum(nhits > 0))
}
```

We can now check the N-filtered sequences of just one sample set for primer hits:
```{r, cntPrimerHits, echo=TRUE, eval=TRUE, tidy=FALSE, cache=TRUE, message=FALSE, warning=FALSE, comment=NA}
library("ShortRead")
rbind(FWD.ForwardReads = sapply(fwdOrients, PrimerHits, fn = metadataRegion$filtF[1]),
      FWD.ReverseReads = sapply(fwdOrients, PrimerHits, fn = metadataRegion$filtR[1]), 
      REV.ForwardReads = sapply(revOrients, PrimerHits, fn = metadataRegion$filtF[1]), 
      REV.ReverseReads = sapply(revOrients, PrimerHits, fn = metadataRegion$filtR[1]))
```
As Expected, the vast majority of forward primer is found in its forward orientation, and in some of the reverse reads in its reverse-complement orientation (due to read-through when the COI region is short). Similarly, the reverse primer is found with its expected orientations. There are some where the primers appear in incorrect orientations and I need to better understand this.   
    

Check out the forward and reverse COI primers for 2022 sequencing sets:
```{r, recordPrimers, echo=TRUE, eval=TRUE, include=TRUE, tidy=FALSE, cache=TRUE}
# COI-Invertebrates: Bfrag:
# Befrag-BF: CCIGAYATRGCITTYCCICG = CCBGAYATRGCBTTYCCBCG
# Befrag-R5-R: GTRATIGCICCIGCIARIAC = GTRATBGCBCCBGCBARBAC
fwdPrimer <- "CCBGAYATRGCBTTYCCBCG"
revPrimer <- "GTRATBGCBCCBGCBARBAC"

library("Biostrings")
AllOrients   <- function(primer) {
     require(Biostrings)
     dna     <- Biostrings::DNAString(primer)
     orients <- c(Forward    = dna, 
                  Complement = Biostrings::complement(dna), 
                  Reverse    = Biostrings::reverse(dna), 
                  RevComp    = Biostrings::reverseComplement(dna))
    return(sapply(orients, toString))
}
fwdOrients <- AllOrients(fwdPrimer)
revOrients <- AllOrients(revPrimer)
fwdOrients
library("ShortRead")
PrimerHits <- function(primer, fn) {
    nhits  <- Biostrings::vcountPattern(primer, ShortRead::sread(readFastq(fn)),
                                        fixed = FALSE)
    return(sum(nhits > 0))
}
library("ShortRead")
rbind(FWD.ForwardReads = sapply(fwdOrients, PrimerHits, fn = metadataRegion$filtF[111]),
      FWD.ReverseReads = sapply(fwdOrients, PrimerHits, fn = metadataRegion$filtR[111]), 
      REV.ForwardReads = sapply(revOrients, PrimerHits, fn = metadataRegion$filtF[111]), 
      REV.ReverseReads = sapply(revOrients, PrimerHits, fn = metadataRegion$filtR[111]))

# set5 - invertebrates - bfrag:
#                  Forward Complement Reverse RevComp
# FWD.ForwardReads    6520          0       0     934
# FWD.ReverseReads     952          0       0    6411
# REV.ForwardReads    1934          0       0    6210
# REV.ReverseReads    5966          0       0    1953
# 
# For this 2022 sequencing read set, row 111 in metadataRegion table, the most primer hits come with the invertebrates Bfrag primer set.
```
    
Set up the paths and fastq file input and output names in preparation for running cutadapt. 
\textbf{Note:} You may need to install cutadapt locally if it is not installed system wide. I chose to use conda to install it locally:
```{r, preCutadapt, echo=TRUE, eval=FALSE, include=TRUE, tidy=FALSE, cache=TRUE}
cutadapt <- "/isilon/cfia-ottawa-fallowfield/users/girouxeml/prog/anaconda3/envs/bio2022/bin/cutadapt"
```

We can use the `system2` call to run shell commands directly from R
```{r, preCutadapt2, echo=TRUE, eval=TRUE, include=TRUE, tidy=FALSE, cache=TRUE}
system2(cutadapt, args = "--help")
```

Create a directory for the cutadapted fastq files, only if it doesn't already exist. Also create the output filenames for the cutadapt-ed fastq files.
```{r, preCutadapt3, echo=TRUE, eval=FALSE, include=TRUE, tidy=FALSE, cache=TRUE}
cutAdaptDir <- file.path(processedFastq, "B_cutadapt", sep = "")
if(!dir.exists(cutAdaptDir)) dir.create(cutAdaptDir)
```
    
Add the paths for cutadapt-processed reads to the full table: 
```{r}
library("data.table")
metadataRegion$cutFqs <- paste(cutAdaptDir, metadataRegion$LibraryName, "_Fcut.fastq.gz", sep = "")
metadataRegion$cutRqs <- paste(cutAdaptDir, metadataRegion$LibraryName, "_Rcut.fastq.gz", sep = "")
```
    
Split the metadataRegion table into one table for pre-2022 reads:
```{r}
metadataPre2022 <- metadataRegion[ which(metadataRegion$SeqPlate !=3)]
```

Now we can proceed to using cutadapt to remove primer sequences at the ends of our reads.     
We'll use dada2:::rc()      
     
An aside:  The ::: is one of 2 possible namespace operators in R. This triple-colon operator acts like a double-colon operator (which selects definitions from a particular namespace), AND allows access to hidden objects. In this command, we are specifying that we want to use the `rc` function that is from the dada2 package, not the `rc` function that may also exist in base R or other packages we possibly loaded. See this page for more on namespace operators in R: http://r-pkgs.had.co.nz/namespace.html      
     
In the following, we use dada2:::rc to get the reverse complement of the primer sequences. The idea is that it can be used to compare each sequence and its reverse complement to a reference sequence(s) in the right orientation, and then choose the orientation that minimizes that distance. See page: https://github.com/benjjneb/dada2/issues/451       
      
         
Cutadapt flag parameter explanations:    
    
Parameter | Definition
--------- | -------------------------------------------------------------
**-g:** | Regular 5' adapter/primer sequence on forward reads
**-a:** | Regular 3' adapter/primer sequence on forward reads
**-G:** | Regular 5' adapter/primer sequence on reverse reads
**-A:** | Regular 3' adapter/primer sequence on reverse reads
**-p:** | Specify paired-end sequencing reads
**-o:** | Specify output files
**-n:** | Number of times to trim when more than one adapter is present in a read
     
Documentation for fine-tuning use of cutadapt is available at: https://cutadapt.readthedocs.io/en/v1.7.1/guide.html 
We pass the primer set F230 with F230R_modN to cutadapt for trimming pre-2022 read sets.     
```{r, cutadapt, echo=TRUE, eval=FALSE, include=TRUE, tidy=FALSE, cache=TRUE}
library("dada2")
# Pre-2022 primer set:
fwdPrimer <- "GGTCAACAAATCATAAAGATATTGG"
revPrimer <- "CTTATRTTRTTTATNCGNGGRAANGC"

fwdPrimerRC <- dada2:::rc(fwdPrimer)
revPrimerRC <- dada2:::rc(revPrimer)
# Trim fwdPrimer and the reverse-complement of the revPrimer off forward reads:
fwdFlags <- paste("-g", fwdPrimer, "-a", revPrimerRC)
# Trim revPrimer and the reverse-complement of the fwdPrimer off reverse reads:
revFlags <- paste("-G", revPrimer, "-A", fwdPrimerRC)
# Run Cutadapt
for(i in seq_along(metadataPre2022$LibraryName)) {
  system2(cutadapt, args = c(fwdFlags, revFlags, "-n", 2,
                             "-o", metadataPre2022$cutFqs[i], "-p", metadataPre2022$cutRqs[i],
                             metadataPre2022$filtF[i], metadataPre2022$filtR[i]))
}
```
\textbf{Note:} There is a lot of output generated to the console when running cutadapt. One warning we may see often is about detection of incomplete adapter sequences:     
     
> `WARNING:`     
>     
> `One or more of your adapter sequences may be incomplete.`     
> `Please see the detailed output above.`    
     
Because our DNA fragments are generated from amplicon sequences and are not random, we can ignore this warning.     
     
As a sanity check, we will count the presence of primers in the first cutadapt-ed sample:
```{r, cntPrimerHits2, echo=-1, eval=TRUE, include=TRUE, tidy=FALSE, cache=TRUE, comment=NA}
library("ShortRead")
rbind(FWD.ForwardReads = sapply(fwdOrients, PrimerHits, fn = metadataPre2022$cutFqs[1]),
      FWD.ReverseReads = sapply(fwdOrients, PrimerHits, fn = metadataPre2022$cutRqs[1]), 
      REV.ForwardReads = sapply(revOrients, PrimerHits, fn = metadataPre2022$cutFqs[1]), 
      REV.ReverseReads = sapply(revOrients, PrimerHits, fn = metadataPre2022$cutRqs[1]))
```

Repeat the cutadapt trimming for 2022 reads:    
Split the metadataRegion to get only 2022 reads:
```{r}
library("data.table")
metadata2022 <- metadataRegion[ which(metadataRegion$SeqPlate==3)]

# Set the other primer set Bfrag:
fwdPrimer <- "CCBGAYATRGCBTTYCCBCG"
revPrimer <- "GTRATBGCBCCBGCBARBAC"

library("dada2")
fwdPrimerRC <- dada2:::rc(fwdPrimer)
revPrimerRC <- dada2:::rc(revPrimer)
# Trim fwdPrimer and the reverse-complement of the revPrimer off forward reads:
fwdFlags <- paste("-g", fwdPrimer, "-a", revPrimerRC)
# Trim revPrimer and the reverse-complement of the fwdPrimer off reverse reads:
revFlags <- paste("-G", revPrimer, "-A", fwdPrimerRC)
# Run Cutadapt
for(i in seq_along(metadata2022$LibraryName)) {
  system2(cutadapt, args = c(fwdFlags, revFlags, "-n", 2,
                             "-o", metadata2022$cutFqs[i], "-p", metadata2022$cutRqs[i],
                             metadata2022$filtF[i], metadata2022$filtR[i]))
}

# Sanity check:
library("ShortRead")
rbind(FWD.ForwardReads = sapply(fwdOrients, PrimerHits, fn = metadata2022$cutFqs[1]),
      FWD.ReverseReads = sapply(fwdOrients, PrimerHits, fn = metadata2022$cutRqs[1]), 
      REV.ForwardReads = sapply(revOrients, PrimerHits, fn = metadata2022$cutFqs[1]), 
      REV.ReverseReads = sapply(revOrients, PrimerHits, fn = metadata2022$cutRqs[1]))
```
Combine the split tables back into a full metadata table:
```{r}
library("data.table")
l <- list(metadataPre2022, metadata2022)
metadataFull <- rbindlist(l, use.names=TRUE)
```


3. filterAndTrimming. Raw read processing.          
If we're going to spend anytime optimizing steps during procesing, it should be on filterring and trimming. Likewise, while we may see a big loss of reads in this step, outside of this step we should see no major loss of reads.     
      
Reads less than 60 bp in length will never overlap for this region and are most likely junk reads, so we will be filtering these reads out.        
We can use the quality plots of the raw reads to guide our trimming and filtering approach. In the plots previously generated we saw that the quality of the forwards reads was much higher than the reverse reads, which is to be expected for Illumina data.  Forward read quality was relatively adequate throughout read length. Reverse read quality dropped off much sooner, with much lower quality after the 180-200 bp mark.  We have the option to truncate our reads in this step, but we need to keep in mind our expected amplicon length for successful merging later on, which depends on a 20-bp overlap between forward and reverse reads.   
We combine these trimming parameters with standard filtering parameters, the most important being the enforcement of a maximum of 2 expected errors per-read (Edgar and Flyvbjerg 2015). Trimming and filtering is performed on paired reads jointly, i.e. both reads must pass the filter for the pair to pass.     
When filtering - even if I remove the option to discard reads with >2 error rate, I may still lose samples where no reads remain after filtering. Given the importance stressed by the referenced paper by Edgar and Flyvbjerg 2015 for filtering reads with higher error rates, I kept it, but increased the allowed errors to 3 per read, instead of 2.  We need to accept that some samples will be lost due to poor sequencing data. We will need to update the metadata table so that the lost sets are removed in a later step.     
    
```{r, filterAndTrimming, echo=TRUE, eval=FALSE, include=TRUE, tidy=FALSE, cache=TRUE}
# Path to the final processed fastq file directory:
filtPathFinal <- file.path(processedFastq, "C_finalProcessed")
if(!dir.exists(filtPathFinal)) dir.create(filtPathFinal)

# Create output filenames for the final filtered and trimmed fastq files:
metadataFull$filtFwd <- paste(filtPathFinal, "/", 
                              metadataFull$LibraryName, "_F_filt.fastq.gz", sep = "")
metadataFull$filtRev <- paste(filtPathFinal, "/",
                              metadataFull$LibraryName, "_R_filt.fastq.gz", sep = "")
```

For this dataset, we will use standard filtering paraments: maxN=0 (DADA2 requires sequences contain no Ns), truncQ = 2,  rm.phix = TRUE and maxEE=2. The maxEE parameter sets the maximum number of “expected errors” allowed in a read, which is a better filter than simply averaging quality scores. Note: We enforce a minLen here, to get rid of spurious very low-length sequences.
      
Run filterAndTrim using cutadapt-ed fastq files as input:      
```{r, filterAndTrimming2, echo=TRUE, eval=FALSE, include=TRUE, tidy=FALSE, linewidth=80, cache=TRUE}
library("dada2")
trimOut <- dada2::filterAndTrim(metadataFull$cutFqs, metadataFull$filtFwd, metadataFull$cutRqs, metadataFull$filtRev,
                                maxN = 0, maxEE = c(3,3), truncQ = 2, 
                                minLen = 50, rm.phix = TRUE, compress = TRUE,
                                matchIDs = TRUE, multithread = TRUE, verbose = TRUE)
```
\pagebreak     
     
Save the workspace image right after this step so it doesn't have to be repeated if R accidentally shuts down:
```{r, saveImage2, eval=FALSE, include=TRUE, echo=TRUE}
save.image(paste(imageDirPath, chptImageA, sep = ""))
```
         
Trimming and filtering parameters explanations:    
    
Parameter | Definition
--------- | -------------------------------------------------------------
**minLen:** | Remove reads with length less than minLen. minLen is enforced after trimming and truncation.
**minQ:** | After truncation, reads that contain a quality score less than minQ will be discarded.
**truncQ:** | Truncate reads at the first instance of a quality score less than or equal to truncQ.
**maxN:** | After truncation, sequences with more than maxN Ns will be discarded.
**maxEE:** | After truncation, reads with higher than maxEE "expected errors" will be discarded. Expected errors are calculated from the nominal definition of the quality score: EE = sum(10^(-Q/10))
**rm.phix:** | If TRUE, discard reads that match against the phiX genome.
**matchIDs:** | Whether to enforce matching between the id-line sequence identifiers of the forward and reverse fastq files. If TRUE, only paired reads that share id fields are output.
**compress:** | Ouput fastq files are gzipped.
**multithread:** | If TRUE, input files are filtered in parallel via mclapply.
**verbose:** | Whether to output status messages.
     
For the full list of available parameters and permitted parameter values see the `filterAndTrim` function in the DADA2 manual page:        
https://rdrr.io/bioc/dada2/man/filterAndTrim.html      
     
4. ggPlotsProcessed. Let's look at the quality plots of our filtered and processed reads for our first 2 samples to see the effects of our trimming parameters:
```{r, ggPlotsProcessed, eval=TRUE, include=TRUE, echo=TRUE, fig.show='hold', message=FALSE, tidy=FALSE, cache=TRUE}
library("dada2")
library("ggplot2")
plotQfwd <- dada2::plotQualityProfile(paste(filtPathFinal, "/", 
                                            metadataFull$LibraryName[1:2], 
                                            "_F_filt.fastq.gz", sep = "")) +
  ggplot2::ggtitle("Quality Plot of Two Samples of Processed Forward Reads") +
  ggplot2::theme(plot.title = element_text(hjust = 0.5))
plotQrev <- dada2::plotQualityProfile(paste(filtPathFinal, "/", 
                                            metadataFull$LibraryName[1:2],
                                            "_R_filt.fastq.gz", sep = "")) +
  ggplot2::ggtitle("Quality Plot of Two Samples of Processed Reverse Reads") +
  ggplot2::theme(plot.title = element_text(hjust = 0.5))

library("cowplot")
cowplot::plot_grid(plotQfwd, plotQrev, nrow = 2)
```
5. keepTrimmed. Update metadata so that samples that did not have any reads that passed filtering are removed from further analysis, to avoid downstream processing errors.   
     
We can take a brief look at the summary or reads in and out for the first 6 samples:
```{r, preKeepTrimmed, eval=TRUE, include=TRUE, echo=TRUE, cache=TRUE, comment=NA}
head(trimOut)
```
\pagebreak      
     
We can order the samples by reads remaining after processing with:
```{r, preKeepTrimmedSorted, eval=TRUE, include=TRUE, echo=TRUE, cache=TRUE, comment=NA}
head(trimOut[order(-trimOut[,2]),]) 
```
The row names we see have retained the fastq file name of the input forward reads, yet the output sums are for both forward and reverse reads. Let's remove the file suffix so that the rownames will apply to both forward and reverse reads.
```{r, updateRows, include=TRUE, echo=TRUE, eval=FALSE, cache=TRUE}
rownames(trimOut) <- gsub("_Fcut.fastq.gz", "", rownames(trimOut))
```

As a precaution, we still check for samples with no remaining reads and update the metadata table for our COI samples using those results. If you receive an error during the dereplication step later on: `Error in open.connection(con, "rb") : cannot open the connection`,
you need to update the files so that those that didn't pass filtering aren't attempted. See issue at:    
https://github.com/benjjneb/dada2/issues/375    
```{r, keepsTrimmed, include=TRUE, echo=TRUE, eval=FALSE, cache=TRUE, tidy=FALSE}
keepTrim <- trimOut[,"reads.out"] > 20 # Or other cutoff

#keepTrim <- keepTrim[keepTrim =="TRUE"]

#names <- which(keepTrim == TRUE, arr.ind = TRUE)

filtFs <- file.path(filtPathFinal, 
                    paste(metadataFull$LibraryName, 
                          "_F_filt.fastq.gz", sep = ""))[keepTrim]
filtRs <- file.path(filtPathFinal, 
                    paste(metadataFull$LibraryName, 
                          "_R_filt.fastq.gz", sep = ""))[keepTrim]

filtNames <- na.omit(basename(filtFs)[keepTrim]) # Using na.omit removes those that failed the minimum read number after filtering.
filtNames <- gsub("_F_filt.fastq.gz", "", filtNames)
```

Run the `keepTrimmed2` chunk. We've updated our list of fastq file names, but we also need to update our metadata table so that rows that had read pairs where a direction no longer had reads after filtering are removed from the metadata table.    
\textbf{Note:} With `data.table` and `keys` look-up, `.()` is an alias to `list()`, which is the `filtNames` we set in the previous chunk.
```{r, keepTrimmed2, include=TRUE, echo=TRUE, eval=FALSE, message=FALSE, cache=TRUE, tidy=FALSE}
library("data.table")
data.table::setkey(metadataFull, LibraryName)
metadatafilt <- metadataFull[.(filtNames)] 
metadatafilt$filtFwd <- paste(filtPathFinal, "/", metadatafilt$LibraryName, 
                              "_F_filt.fastq.gz", sep = "")
metadatafilt$filtRev <- paste(filtPathFinal, "/", metadatafilt$LibraryName,
                              "_R_filt.fastq.gz", sep = "")
```

6. splitRunsMetadata. Pre-2022 and 2022 COI sequencing libraries make up 2 separate sequencing runs.
```{r, splitRunsMetadata, include=TRUE, echo=TRUE, eval=FALSE, message=FALSE, cache=TRUE}
library("data.table")
data.table::setkey(metadatafilt, "SeqPlate")
run <- unique(metadatafilt$SeqPlate)
metadatafiltPlate1 <- metadatafilt[run[2]] #notRecorded set = pre-2022
metadatafiltPlate2 <- metadatafilt[run[1]] #3 = 2022 runs
```

```{r, saveImage3, eval=FALSE, echo=TRUE, include=TRUE}
save.image(paste(imageDirPath, chptImageA, sep = ""))
```
